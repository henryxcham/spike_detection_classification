{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57926404",
   "metadata": {},
   "source": [
    "# 2. spike classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1eab2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from src.utils import load_and_concatenate_npy, normalize_data\n",
    "from src.models import CNNClassifier, GRUClassifier\n",
    "from src.train import evaluate_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a04ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preparation (for evaluation)\n",
    "background_file_paths_test = [\n",
    "    '../data/spikeshannel_background_40.npy'\n",
    "]\n",
    "spikes_file_paths_test = [\n",
    "    '../data/spikes/channel_spikes_40.npy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd587f9d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_test_dataset(background_paths, spikes_paths):\n",
    "    background_array = load_and_concatenate_npy(background_paths)\n",
    "    spikes_array = load_and_concatenate_npy(spikes_paths)\n",
    "    \n",
    "    X_np = np.concatenate([background_array, spikes_array], axis=0)\n",
    "    y_np = np.concatenate([\n",
    "        np.zeros(background_array.shape[0]), \n",
    "        np.ones(spikes_array.shape[0])\n",
    "    ], axis=0)\n",
    "    \n",
    "    X_tensor = torch.from_numpy(X_np).float()\n",
    "    y_tensor = torch.from_numpy(y_np).long()\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32180ba7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = create_test_dataset(background_file_paths_test, spikes_file_paths_test)\n",
    "train_mean = X_test.mean()\n",
    "train_std = X_test.std()\n",
    "X_test_normalized = normalize_data(X_test, train_mean, train_std).unsqueeze(1)\n",
    "test_dataset = TensorDataset(X_test_normalized, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2fd60f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate CNN model\n",
    "cnn_model = CNNClassifier(input_size=X_test.shape[1], num_classes=2)\n",
    "print(\"Evaluating CNN model...\")\n",
    "evaluate_model(cnn_model, 'best_cnn_model.pth', test_loader, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc1791",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate GRU model\n",
    "gru_model = GRUClassifier(input_size=X_test.shape[1], num_classes=2)\n",
    "print(\"\\nEvaluating GRU model...\")\n",
    "evaluate_model(gru_model, 'best_gru_model.pth', test_loader, 'Test')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
